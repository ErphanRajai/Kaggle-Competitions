{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyObogqqPO6SKrjM6y80qL01"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Solving [Kaggles's Digit Recognizer Competition](https://www.kaggle.com/competitions/digit-recognizer/overview)\n","\n","- thanks to [This NoteBook](https://www.kaggle.com/code/swapnilkumarnorge/mynameistorchpytorch) that gave me understanding to understand how to look at this problem through PyTorch's eyes. checking the link is totally recommended.\n","\n","I have tried to solve this problem using other great libraries such as Scikit-Learn and tensorflow with 98% accuracy. which is awesome.\n","\n","But with PyTorch's Custom CNN model's and internets Great help this time i've reached 99.36% accuracy which is beyond great!\n"],"metadata":{"id":"iN6ltutJzzD_"}},{"cell_type":"markdown","source":["## 01. Importing necessary libraries"],"metadata":{"id":"2gwS6zKw1iyc"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"ErkijnO2jW_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 0.1.1 Setting up Device agnostic Code"],"metadata":{"id":"vCljgzEp1scz"}},{"cell_type":"code","source":["device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfgNfcfjk3cB","executionInfo":{"status":"ok","timestamp":1726384990550,"user_tz":-210,"elapsed":5,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"65925d77-8afc-44cc-e26b-a2e918e7a856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["## 0.2 Seperating and Reshaping Train data and Test Data\n","* Creating a labels variant as *Y_train*\n","* Removing *Labels* from the train data\n","* Normilalizing the data\n","* Reshape images into (num_imgs, color_channels, height_pixels, width_pixels)"],"metadata":{"id":"cjS0I1bA1y_T"}},{"cell_type":"code","source":["train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","\n","Y_train = train[\"label\"]\n","X_train = train.drop(labels=[\"label\"], axis=1)\n","\n","# Normalize the data\n","X_train = X_train / 255.0\n","test = test / 255.0\n","\n","# Reshape images into 28x28 pixels and 1 channel\n","X_train = X_train.values.reshape(-1, 1, 28, 28)\n","test = test.values.reshape(-1, 1, 28, 28)\n","print(X_train.shape)\n","print(test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hsb6t0Vdmo5l","executionInfo":{"status":"ok","timestamp":1726384993986,"user_tz":-210,"elapsed":3438,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"7baccffc-3472-468b-cedd-acb1410d32c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(42000, 1, 28, 28)\n","(28000, 1, 28, 28)\n"]}]},{"cell_type":"markdown","source":["## 0.2.1 Looking at what we're dealing with"],"metadata":{"id":"lDixsyie2U1y"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","X = X_train[1].reshape(28, 28 ,1) #(height, width, color_channels) for matplotlib\n","plt.figure(figsize=(5, 3))\n","plt.imshow(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"GGxvr_G3oCwv","executionInfo":{"status":"ok","timestamp":1726384993987,"user_tz":-210,"elapsed":7,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"50e996c2-0328-4a08-8db3-e88ac4be6f53"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7e63ee459e10>"]},"metadata":{},"execution_count":24},{"output_type":"display_data","data":{"text/plain":["<Figure size 500x300 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## 0.2.2 Changing our Data's Dtype into `torch.float32` and our labels into `torch.long()`"],"metadata":{"id":"EFufvVtA2fQ8"}},{"cell_type":"code","source":["Y_train = torch.tensor(Y_train.values, dtype=torch.long)\n","\n","# Split the train and validation sets\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=2, stratify=Y_train)\n","\n","# Convert numpy arrays to tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_val = torch.tensor(X_val, dtype=torch.float32)\n","Y_train = torch.tensor(Y_train, dtype=torch.float32)\n","Y_val = torch.tensor(Y_val, dtype=torch.float32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IB0MJoYGoE-_","executionInfo":{"status":"ok","timestamp":1726384994354,"user_tz":-210,"elapsed":372,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"4330bd6b-1f5f-4cd4-f5d6-2dfc2504afc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-dec8d5b81d1f>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  Y_train = torch.tensor(Y_train, dtype=torch.float32)\n","<ipython-input-25-dec8d5b81d1f>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  Y_val = torch.tensor(Y_val, dtype=torch.float32)\n"]}]},{"cell_type":"markdown","source":["## 0.3 Creating a Custom dataset Class"],"metadata":{"id":"TXo6r1kL2yW3"}},{"cell_type":"code","source":["class MNISTDataset(Dataset):\n","    def __init__(self, images, labels=None, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","\n","        if self.transform:\n","            # Apply the transformations\n","            image = self.transform(image)\n","        else:\n","            # If no transform is specified just return the imge\n","            image = image\n","\n","        if self.labels is not None:\n","            return image, self.labels[idx]\n","        else:\n","            return image"],"metadata":{"id":"xKiAJGbfskL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Dft79qSJ23zL"}},{"cell_type":"markdown","source":["## 0.3.1 Creating Data Transforms , Creating DataSets and Truning them into DataLoaders"],"metadata":{"id":"cokcehmH3ASI"}},{"cell_type":"code","source":["# Define transforms for Augmentation\n","transform = transforms.Compose([\n","    transforms.RandomRotation(10),\n","    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n","    transforms.ToTensor(),\n","])\n","\n","class ToTensorTransform:\n","    def __call__(self, x):\n","        return x\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomRotation(10),\n","    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n","    ToTensorTransform()\n","])\n","\n","val_transform = ToTensorTransform()\n","test_transform = ToTensorTransform()\n","\n","# Create PyTorch Datasets\n","train_dataset = MNISTDataset(X_train, Y_train, transform=train_transform)\n","val_dataset = MNISTDataset(X_val, Y_val, transform=val_transform)\n","test_dataset = MNISTDataset(test, transform=test_transform)\n","\n","# DataLoader\n","BATCH_SIZE = 64\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"id":"l_JfVw5ot9PQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 0.4 Creating our Convolutional Neural Network (CNN) model"],"metadata":{"id":"3-ZdBjPM3LgL"}},{"cell_type":"code","source":["# Define the CNN model with batch normalization\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        # Convolutional layers with batch normalization\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.25)\n","\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.dropout_conv = nn.Dropout(0.25)\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(64*7*7, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.dropout_fc = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.pool(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.pool(x)\n","        x = self.dropout_conv(x)\n","\n","        x = x.view(-1, 64*7*7)\n","        x = self.fc1(x)\n","        x = self.dropout_fc(x)\n","        x = self.fc2(x)\n","        return x # Return raw logits\n","\n","# Move the model to the target device\n","model = CNN().to(device)\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCz2EYHAbuDe","executionInfo":{"status":"ok","timestamp":1726385766677,"user_tz":-210,"elapsed":416,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"6fb43e70-243c-4160-80a9-e4de60179f63"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout): Dropout(p=0.25, inplace=False)\n","  (conv3): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (dropout_conv): Dropout(p=0.25, inplace=False)\n","  (fc1): Sequential(\n","    (0): Linear(in_features=3136, out_features=256, bias=True)\n","    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (dropout_fc): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## 0.4.1 Setting up Optimizer and Loss Function\n"],"metadata":{"id":"1Xo-indxbzTM"}},{"cell_type":"code","source":["# Deefine optimizer with weight decay (L2 Regularization)\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","\n","# Define the learning rate scheduler\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', patience=3, factor=0.5, min_lr=1e-5\n",")\n","\n","# Loss Function\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"ea9p2IZifdMl","executionInfo":{"status":"ok","timestamp":1726385769172,"user_tz":-210,"elapsed":372,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["## 0.5 Training the data"],"metadata":{"id":"H_Ahp-GohCWF"}},{"cell_type":"code","source":["epochs = 30\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in  train_loader:\n","        images, labels = images.to(device), labels.long().to(device) # Convert labels to long\n","\n","        optimizer.zero_grad()\n","        outputs=model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = 100 * correct / total\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDZkT8HagHxr","executionInfo":{"status":"ok","timestamp":1726386923811,"user_tz":-210,"elapsed":998045,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"3ae849e2-cc0d-4020-b6de-fa8cf6c79cc8"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.2749, Accuracy: 92.16%\n","Epoch [2/30], Loss: 0.1070, Accuracy: 96.78%\n","Epoch [3/30], Loss: 0.0879, Accuracy: 97.36%\n","Epoch [4/30], Loss: 0.0761, Accuracy: 97.73%\n","Epoch [5/30], Loss: 0.0679, Accuracy: 97.89%\n","Epoch [6/30], Loss: 0.0609, Accuracy: 98.12%\n","Epoch [7/30], Loss: 0.0565, Accuracy: 98.32%\n","Epoch [8/30], Loss: 0.0560, Accuracy: 98.22%\n","Epoch [9/30], Loss: 0.0518, Accuracy: 98.32%\n","Epoch [10/30], Loss: 0.0473, Accuracy: 98.52%\n","Epoch [11/30], Loss: 0.0441, Accuracy: 98.62%\n","Epoch [12/30], Loss: 0.0457, Accuracy: 98.64%\n","Epoch [13/30], Loss: 0.0457, Accuracy: 98.62%\n","Epoch [14/30], Loss: 0.0404, Accuracy: 98.74%\n","Epoch [15/30], Loss: 0.0391, Accuracy: 98.78%\n","Epoch [16/30], Loss: 0.0390, Accuracy: 98.73%\n","Epoch [17/30], Loss: 0.0402, Accuracy: 98.75%\n","Epoch [18/30], Loss: 0.0369, Accuracy: 98.89%\n","Epoch [19/30], Loss: 0.0360, Accuracy: 98.87%\n","Epoch [20/30], Loss: 0.0353, Accuracy: 98.85%\n","Epoch [21/30], Loss: 0.0340, Accuracy: 99.00%\n","Epoch [22/30], Loss: 0.0353, Accuracy: 98.90%\n","Epoch [23/30], Loss: 0.0340, Accuracy: 98.93%\n","Epoch [24/30], Loss: 0.0359, Accuracy: 98.90%\n","Epoch [25/30], Loss: 0.0310, Accuracy: 99.02%\n","Epoch [26/30], Loss: 0.0311, Accuracy: 99.02%\n","Epoch [27/30], Loss: 0.0330, Accuracy: 98.92%\n","Epoch [28/30], Loss: 0.0313, Accuracy: 99.11%\n","Epoch [29/30], Loss: 0.0301, Accuracy: 99.04%\n","Epoch [30/30], Loss: 0.0309, Accuracy: 99.04%\n"]}]},{"cell_type":"markdown","source":["## 0.5.1 Creating a Validation loop so our `val_dataset` runs through it"],"metadata":{"id":"tp3KQEA_3Y5C"}},{"cell_type":"code","source":["# Validation Loop\n","model.eval()\n","val_running_loss = 0.0\n","val_correct = 0\n","val_total = 0\n","with torch.inference_mode():\n","    for images, labels in val_loader:\n","        images, labels = images.to(device), labels.long().to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        val_running_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        val_total += labels.size(0)\n","        val_correct += (predicted==labels).sum().item()\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = 100 * val_correct / val_total\n","\n","    print(f\"Validation Loss : {val_loss:.4f}, Validation Accuracy : {val_acc:.2f}%\")\n","    # Step the scheduler\n","    scheduler.step(val_loss)"],"metadata":{"id":"Yy4GLETWkWaW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726387659529,"user_tz":-210,"elapsed":340,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}},"outputId":"9f58e2a8-f5c9-48fb-af67-0057506b328a"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss : 0.0220, Validation Accuracy : 99.36%\n"]}]},{"cell_type":"markdown","source":["## 0.6 Saving the model"],"metadata":{"id":"gaBHqpde3hDZ"}},{"cell_type":"code","source":["# Saving the model\n","torch.save(model.state_dict(), \"DigitRecognizer.pth\")\n","\n","# Make predicitions on the test set\n","model.eval()\n","test_preds = []\n","with torch.inference_mode():\n","    for images in test_loader:\n","        images = images.float().to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        test_preds.extend(predicted.cpu().numpy())"],"metadata":{"id":"JJ5JDA-Oue9M","executionInfo":{"status":"ok","timestamp":1726387904218,"user_tz":-210,"elapsed":1347,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["## 0.6.1 Saving our Predictions"],"metadata":{"id":"UpRDHcPX3l0T"}},{"cell_type":"code","source":["# Saving predictions into a submission file\n","submission = pd.DataFrame({\n","    'ImageId' : list(range(1, len(test_preds)+1)),\n","    'Label' : test_preds\n","})\n","submission.to_csv(\"MyPredictions.csv\", index=False)"],"metadata":{"id":"4u9__9UlvGqK","executionInfo":{"status":"ok","timestamp":1726388023943,"user_tz":-210,"elapsed":330,"user":{"displayName":"Its Erphan","userId":"06973834403098047756"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["## Final Note\n","i have to once again thank [this notebook](https://www.kaggle.com/code/swapnilkumarnorge/mynameistorchpytorch) because it gave me enough confidence to start solving more and more complex models with PyTorch and finally start my kaggle competition journey\n","\n","i sadly couldn't find any other works from this notebooks author."],"metadata":{"id":"2o82nSmnvplc"}},{"cell_type":"code","source":[],"metadata":{"id":"RDyMXtBD4M62"},"execution_count":null,"outputs":[]}]}